{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a6d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFECV, RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70be548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Energy Efficiency Dataset from UCI ML Repository\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx'\n",
    "df = pd.read_excel(url)\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = df.iloc[:, :-2]  # Select all columns except the last two (features)\n",
    "y = df.iloc[:, -1:]  # Select the label from the last two columns (target variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef649e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759d674c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    float64\n",
       "X2    float64\n",
       "X3    float64\n",
       "X4    float64\n",
       "X5    float64\n",
       "X6      int64\n",
       "X7    float64\n",
       "X8      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deedf1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing\n",
    "X = (X - X.mean())/X.std(ddof=1)\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad03185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8176e71",
   "metadata": {},
   "source": [
    "## using Sklearn RFE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3426346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "def perform_rfe(X_train, y_train, X_test, model):\n",
    "    # Create an RFE object with 5-fold cross-validation\n",
    "    rfe = RFECV(estimator=model, cv=5, verbose=0, scoring='neg_mean_absolute_percentage_error')\n",
    "\n",
    "    # Perform feature selection on the training data\n",
    "    X_train_selected = rfe.fit_transform(X_train, y_train)\n",
    "    \n",
    "    print(\"no of features selected:\", rfe.n_features_)\n",
    "    print(\"X selected: \", X_train_selected.shape)\n",
    "    # Get the support of the selected features\n",
    "    support = rfe.support_\n",
    "    print(rfe.support_)\n",
    "    # Get the names of the selected features\n",
    "    selected_features = [feature_names[i] for i in range(len(feature_names)) if support[i]]\n",
    "    \n",
    "    cv_results = rfe.cv_results_\n",
    "    mean_test_scores = cv_results['mean_test_score']\n",
    "    best_idx = np.argmax(mean_test_scores)\n",
    "    print(\"mean CV scores:\", mean_test_scores)\n",
    "    \n",
    "    # Apply feature selection to the testing data\n",
    "    X_test_selected = rfe.transform(X_test)\n",
    "\n",
    "    #print(\"ranks for features. lower is better.\")\n",
    "    #print(rfe.ranking_)\n",
    "    return X_train_selected, X_test_selected, selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdafb3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of features selected: 6\n",
      "X selected:  (614, 6)\n",
      "[ True  True  True  True  True False  True False]\n",
      "mean CV scores: [-0.13384525 -0.12867851 -0.12880245 -0.11787789 -0.1066102  -0.08950637\n",
      " -0.09044526 -0.09053393]\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "linear_model = LinearRegression()\n",
    "# Perform RFE with Linear Regression model\n",
    "X_train_rfe_linear, X_test_rfe_linear, selected_features_rfe_linear = perform_rfe(X_train, y_train, X_test, linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "683cc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe mean CV scores, the scores are for best 1,2,3...8 feature models. 6 features option has the highest -MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1141c6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of features selected: 8\n",
      "X selected:  (614, 8)\n",
      "[ True  True  True  True  True  True  True  True]\n",
      "mean CV scores: [-0.11768132 -0.08611914 -0.03904611 -0.03785958 -0.04035149 -0.03785365\n",
      " -0.0375911  -0.03686154]\n"
     ]
    }
   ],
   "source": [
    "# Perform RFE with Decision Tree model\n",
    "tree_model = DecisionTreeRegressor()\n",
    "X_train_rfe_tree, X_test_rfe_tree, selected_features_rfe_tree = perform_rfe(X_train, y_train, X_test, tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "107864d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe mean CV scores, the scores are for best 1,2,3...8 feature models. 8 features option has the highest -MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e55f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(sklearn.metrics.SCORERS.keys()) shows scoring options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc0d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X1', 'X2', 'X3', 'X4', 'X5', 'X7']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_rfe_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02bb85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rfe_linear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97dd29c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X1', 'X2', 'X3', 'X6', 'X7', 'X8']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_rfe_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdce96",
   "metadata": {},
   "source": [
    "## using SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e1d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['X1', 'X2', 'X3', 'X4', 'X5']\n",
      "Model Score: 0.8541115688411918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Create the SelectKBest object\n",
    "k = 5  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "\n",
    "# Fit the selector to the training data\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_features]\n",
    "\n",
    "# Transform the training and testing data to keep only the selected features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train a linear regression model on the selected features\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test_selected, y_test)\n",
    "\n",
    "# Print the selected features and the model score\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "print(\"Model Score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f46e0b",
   "metadata": {},
   "source": [
    "## using RFE (without CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ceed6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=None)\n",
    "\n",
    "# Evaluate model performance for different numbers of features\n",
    "scores = []\n",
    "num_features = range(1, X.shape[1] + 1)\n",
    "\n",
    "for k in num_features:\n",
    "    rfe.n_features_to_select = k\n",
    "    X_selected = rfe.fit_transform(X, y)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_selected, y)\n",
    "    y_pred = model.predict(X_selected)\n",
    "    score = r2_score(y, y_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "# Find the optimal number of features based on the highest score\n",
    "optimal_num_features = num_features[np.argmax(scores)]\n",
    "\n",
    "# Use the optimal_num_features for feature selection\n",
    "rfe.n_features_to_select = optimal_num_features\n",
    "X_selected = rfe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a752ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of features: 8\n",
      "r square for best models with varying no of features: [0.7439866432524689, 0.7774655257789087, 0.7719628145944, 0.822680504627051, 0.8435885321091069, 0.8872078371928117, 0.8869802289997921, 0.8876871197455591]\n",
      "based on highest r square, best no of features: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"no of features:\",len(scores))\n",
    "print(\"r square for best models with varying no of features:\", scores)\n",
    "print(\"based on highest r square, best no of features:\", optimal_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f5b31",
   "metadata": {},
   "source": [
    "#### r square always increases with feature count. so it is not a good way to compare models with different no of features. modify this code to get MAPE or RMSE for each model. or use adjusted r square."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df5c5e",
   "metadata": {},
   "source": [
    "## using mlxtend SFS (forward selection and backward elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcde4577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.2s finished\n",
      "\n",
      "[2023-05-29 10:47:52] Features: 1/8 -- score: 0.8000636362172259[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.2s finished\n",
      "\n",
      "[2023-05-29 10:47:52] Features: 2/8 -- score: 0.842959316574895[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.2s finished\n",
      "\n",
      "[2023-05-29 10:47:52] Features: 3/8 -- score: 0.8772456518805024[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "\n",
      "[2023-05-29 10:47:52] Features: 4/8 -- score: 0.8824196887420308[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "\n",
      "[2023-05-29 10:47:52] Features: 5/8 -- score: 0.8837636872177667[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7']\n",
      "Model Score: 0.8931291583109783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "\n",
      "[2023-05-29 10:47:53] Features: 6/8 -- score: 0.8838408781438556[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "\n",
      "[2023-05-29 10:47:53] Features: 7/8 -- score: 0.8838463795204088[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "\n",
      "[2023-05-29 10:47:53] Features: 8/8 -- score: 0.8820719695504179"
     ]
    }
   ],
   "source": [
    "# Create the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create the SequentialFeatureSelector object\n",
    "sfs = SequentialFeatureSelector(estimator=model, k_features='best', forward=True, verbose=2, scoring='r2')\n",
    "\n",
    "# Perform feature selection\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = sfs.k_feature_idx_\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "# Transform the training and testing data to keep only the selected features\n",
    "X_train_selected = sfs.transform(X_train)\n",
    "X_test_selected = sfs.transform(X_test)\n",
    "\n",
    "# Train a linear regression model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test_selected, y_test)\n",
    "\n",
    "# Print the selected features and the model score\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "print(\"Model Score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b2417",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96974a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Feature Importances:\n",
      "X1: 1.2513473800880708\n",
      "X5: 1.0951274284940549\n",
      "X4: 0.37920500791286105\n",
      "X2: 0.3747715850219111\n",
      "X7: 0.07377439196330729\n",
      "X3: 0.0005505470273164282\n",
      "X6: 0.0005379491454088669\n",
      "X8: 0.00021772242474873415\n",
      "\n",
      "Decision Tree Regression Feature Importances:\n",
      "X5: 2.2309198781474633\n",
      "X2: 0.2616295694683056\n",
      "X7: 0.07453719066251532\n",
      "X1: 0.03644309018380656\n",
      "X8: 0.03196278188739378\n",
      "X3: 0.02502556819797357\n",
      "X6: 0.01880653559814697\n",
      "X4: 0.0001316841934896118\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform permutation importance for Linear Regression\n",
    "linear_results = permutation_importance(linear_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "linear_importances = linear_results.importances_mean\n",
    "\n",
    "# Sort the linear feature importances in descending order\n",
    "linear_sorted_indices = linear_importances.argsort()[::-1]\n",
    "linear_sorted_feature_names = feature_names[linear_sorted_indices]\n",
    "linear_sorted_importances = linear_importances[linear_sorted_indices]\n",
    "\n",
    "# Print the sorted linear feature importances\n",
    "print(\"Linear Regression Feature Importances:\")\n",
    "for feature, importance in zip(linear_sorted_feature_names, linear_sorted_importances):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "# Decision Tree Regression\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform permutation importance for Decision Tree Regression\n",
    "tree_results = permutation_importance(tree_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "tree_importances = tree_results.importances_mean\n",
    "\n",
    "# Sort the decision tree feature importances in descending order\n",
    "tree_sorted_indices = tree_importances.argsort()[::-1]\n",
    "tree_sorted_feature_names = feature_names[tree_sorted_indices]\n",
    "tree_sorted_importances = tree_importances[tree_sorted_indices]\n",
    "\n",
    "# Print the sorted decision tree feature importances\n",
    "print(\"\\nDecision Tree Regression Feature Importances:\")\n",
    "for feature, importance in zip(tree_sorted_feature_names, tree_sorted_importances):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c80cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
